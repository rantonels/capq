<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>CAPQ - </title>
  <link href='http://fonts.googleapis.com/css?family=Quattrocento' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Quattrocento+Sans' rel='stylesheet' type='text/css'>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="../style/style.css">
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


</head>
<body>

<div class="col-md-12 qheader">
	<h2 class="text-center">CAPQ - <a href="../">back to index</a></h1>

</div>



<div class="container">
	<div class="col-md-12">


		<h1 id="td1---how-are-negative-temperature-possible-why-are-they-hotter-than-any-positive-temperature-how-were-they-acheived">TD1 - How are negative temperature possible? Why are they hotter than any positive temperature? How were they acheived?</h1>
<p><em>this answer provided by <a href="https://reddit.com/u/Midtek">/u/Midtek</a></em></p>
<p>For the answer to this question, you need to understand the definition of temperature (<span class="math">\(T\)</span>), from first principles, in statistical mechanics. That is, temporarily forget about temperature having anything to do with average energy of gas molecules or feelings of hot and cold. There is a quantity, I would say more fundamental than <span class="math">\(T\)</span>, called thermodynamic beta (<span class="math">\(\beta\)</span>). <span class="math">\(\beta\)</span> is defined in terms of entropy and energy. Suppose we have a system with a fixed number of particles (molecules, atoms, whatever) which is constrained to be in a fixed a volume. This system has an entropy <span class="math">\(S\)</span> and a total energy <span class="math">\(E\)</span>. Let's take a moment to describe <span class="math">\(S\)</span> in a fundamental way.</p>
<p>The entropy is meant to be a measure of the possible number of microstates that are consistent with a given macrostate. What does that even mean? Let's consider a very elementary example. Suppose our &quot;particles&quot; are really switches, which are either on or off. If a switch is on, it has energy <span class="math">\(e = +1\)</span>. If a switch is off, it has energy <span class="math">\(e = -1\)</span>. If there are a total of <span class="math">\(N\)</span> switches, the microstate of the system is simply an N-tuple of <span class="math">\(+1\)</span>s and <span class="math">\(-1\)</span>s, which indicate which switches are on and off. So for a 3-switch system, a microstate of <span class="math">\((1,1,-1)\)</span> tells us that the first two switches are on and the last is off. A microstate of <span class="math">\((-1,1,-1)\)</span> tells us that the first and last are off and the second is on.</p>
<p>The macrostate in this example is the total energy <span class="math">\(E\)</span>. What if I told you that there were <span class="math">\(N\)</span> switches and the total energy was <span class="math">\(E = N\)</span>? Well... with a little bit of thought, you can see that the only possible way for that to happen is if every single switch were on. So we say that there is one microstate associated to the macrostate <span class="math">\(E = N\)</span>. Okay, let's consider other macrostates. For simplicity, let's put <span class="math">\(N = 2\)</span>. What are the possible microstates? There are four:</p>
<p><span class="math">\[
\begin{aligned}
    (-1,-1) &amp; &amp; \text{total energy} \; &amp; E = -2 \\
    (-1,+1) &amp; &amp; \text{total energy} \; &amp; E = 0 \\
    (+1,-1) &amp; &amp; \text{total energy} \; &amp; E = 0 \\
    (+1,+1) &amp; &amp; \text{total energy} \; &amp; E = 2 \\
\end{aligned}
\]</span></p>
<p>So we see that there are three possible macrostates, and that the macrostate <span class="math">\(E = 0\)</span> actually has two microstates associated to it. We give the count of the possible microstates a special name and symbol, <span class="math">\(\Omega\)</span>. So we define <span class="math">\(\Omega(E)\)</span> to be the number of microstates that are consistent with the given macrostate (total energy <span class="math">\(E\)</span>). (Technically, <span class="math">\(\Omega\)</span> also depends on <span class="math">\(N\)</span>, but since we will later want to keep <span class="math">\(N\)</span> fixed anyway, I will suppress that dependence.) So in our two-switch example, the function <span class="math">\(\Omega\)</span> has the following values:</p>
<p><span class="math">\[
\begin{aligned}
    \Omega(-2)  &amp; = 1\\
    \Omega(0)   &amp; = 2\\
    \Omega(2)   &amp; = 1
\end{aligned}
\]</span></p>
<p>The entropy <span class="math">\(S\)</span> of the system, which is considered a function of the total energy <span class="math">\(E\)</span>, is then defined to be <span class="math">\(S = \ln(\Omega)\)</span>. (That's the natural logarithm of <span class="math">\(\Omega\)</span>.) The reason we define <span class="math">\(S\)</span> to be the logarithm and not just work with <span class="math">\(\Omega\)</span> directly is a bit subtle. It turns out that with this definition, S is an additive function of independent systems. So if we wanted to know the total entropy of a system that consists of two independent, closed systems, it is just the sum of the entropies for each individual system. It makes math later on much nicer.</p>
<p>I should mention that this definition of entropy is called the &quot;Boltzmann entropy&quot;, and you will encounter other definitions of entropy in texts and papers. Some of the definitions can be shown to be equivalent under certain assumptions. From a statistical point of view, the definition that I have described is fundamental. Also note that you will see this formula written as <span class="math">\(S = k_B \ln(\Omega)\)</span>, where k is Boltzmann's constant. The constant serves only to give entropy units, since otherwise S is dimensionless. This then gives temperature units of kelvins (instead of joules), and really only serves to rescale temperature. The constant <span class="math">\(k_B\)</span> appears in formulas only in the combination <span class="math">\(S/k_B\)</span> and <span class="math">\(k_B T\)</span>, and so the constant is absolutely unnecessary. For simplicity, I will set <span class="math">\(k_B = 1\)</span>. (So entropy <span class="math">\(S\)</span> is dimensionless and temperature <span class="math">\(T\)</span> will end up with units of energy.)</p>
<p>Okay, so hopefully that all makes sense. Entropy <span class="math">\(S\)</span> is ultimately a measure of the number of microstates consistent with a total energy <span class="math">\(E\)</span>. There is one more element of this story. In our model we also assume that each microstate is equally probable. So, for instance, in our two-switch model, the probability that <span class="math">\(E = 0\)</span> is <span class="math">\(\frac{1}{2}\)</span>. The probably that <span class="math">\(E = -2\)</span> is <span class="math">\(\frac{1}{4}\)</span>. Hence, since entropy counts the number of microstates, the maximum entropy corresponds to the most likely total energy.</p>
<p>So where does beta and temperature fit into all of this? Well, we define beta as</p>
<p><span class="math">\[ \beta = \frac{dS}{dE} \]</span></p>
<p>That is, beta is the derivative of <span class="math">\(S\)</span> with respect to <span class="math">\(E\)</span>. Again, we assume the number of particles <span class="math">\(N\)</span> is constant. (In models of real gas molecules, for instance, we would also assume the volume of the container is constant. For our switch model, the volume is meaningless.) Technically, since <span class="math">\(N\)</span> is an integer, this derivative doesn't really exist. But we can assume that <span class="math">\(N\)</span> is large enough so that <span class="math">\(S\)</span> is approximately a continuous function of <span class="math">\(E\)</span>, for which the derivative makes sense. To understand the significance of <span class="math">\(\beta\)</span>, in terms of <em>energy flow</em>, takes a bit more math, but we can consider a very simple example to see what's going on. Suppose we have system <span class="math">\(A\)</span> and system <span class="math">\(B\)</span>, which are independent, but allowed to exchange energy with each other.</p>
<p>Suppose both systems are also in a state far from the entropy maximum. We have not introduced any dynamical element to our model, but it is plausible to think that the total system will move toward a more likely macrostate. So in this plausible scenario, if the systems exchange energy <span class="math">\(dE\)</span>, the energies of the systems and the total entropy change according to:</p>
<p><span class="math">\[ E_A \rightarrow E_A+dE \]</span></p>
<p><span class="math">\[ E_B \rightarrow E_B-dE \]</span></p>
<p><span class="math">\[  S \rightarrow S + dE(\beta_A-\beta_B)+\ldots \]</span></p>
<p>(The &quot;<span class="math">\(\ldots\)</span>&quot; means terms that involve <span class="math">\((dE)^2\)</span> and higher powers. This is essentially a Taylor expansion of the entropy <span class="math">\(S\)</span>.) So in the plausible scenario of the entropy increasing (since the system moves to a more likely macrostate), we must have that <span class="math">\(dE(\beta_A-\beta_B)\)</span> is positive, which means that <span class="math">\(dE\)</span> and <span class="math">\((\beta_A-\beta_B)\)</span> have the same sign.</p>
<p>In other words, energy is more likely to flow from <span class="math">\(A\)</span> to <span class="math">\(B\)</span> if <span class="math">\(\beta_A&lt;\beta_B\)</span>. And here we finally see the physical significance of <span class="math">\(\beta\)</span>. If we define our sense of &quot;hot&quot; and &quot;cold&quot; in terms of the direction of likely energy flow, then we see that lower values of <span class="math">\(\beta\)</span> correspond to hotter systems. The coldest possible system has <span class="math">\(\beta = +\infty\)</span>, the hottest possible system has <span class="math">\(\beta = -\infty\)</span>. In between, the ordering of &quot;hot&quot; and &quot;cold&quot; is completely monotone.</p>
<p>So where the hell does temperature come into this? Well, temperature <span class="math">\(T\)</span> is defined as</p>
<p><span class="math">\[    T = 1/\beta\]</span></p>
<p>Note that since <span class="math">\(\beta\)</span> is allowed to be 0, there is a singularity in the definition of <span class="math">\(T\)</span>, which makes the entire concept of hot and cold very confusing in terms of <span class="math">\(T\)</span>. So the coldest possible temperature is <span class="math">\(T = 0_+\)</span>, and if we increase <span class="math">\(T\)</span> then it gets hotter until <span class="math">\(T = +\infty\)</span>. The next hotter temperature is then <span class="math">\(T = -\infty\)</span>! Increasing <span class="math">\(T\)</span> again, it gets hotter still until we reach the hottest possible temperature <span class="math">\(T = 0_-\)</span>. So we are used to thinking of higher values of <span class="math">\(T\)</span> as hotter, but actually all negative values of <span class="math">\(T\)</span> are hotter than all positive values of <span class="math">\(T\)</span>.</p>
<p>A natural question might be &quot;well why not just use <span class="math">\(\beta\)</span>&quot;? The answer is, well, that we do. However, in many applications, the number <span class="math">\(T\)</span> is actually more useful. For instance, for an ideal gas, <span class="math">\(T\)</span> can be interpreted in terms of the average kinetic energy of the molecules. Many formulas of thermodynamics are a bit more intuitive or in a more aesthetic form. But, perhaps most important, is that realistic systems never have a negative value of <span class="math">\(T\)</span>. And that goes into your second question on how negative temperatures were achieved.</p>
<p>I have (hopefully) satisfactorily explained how negative temperatures are hotter than infinite temperature. I am afraid that I cannot give an answer to your second question (how were negative temperatures achieved) in as much detail. Note that with our definition of beta <span class="math">\(\beta = 0\)</span> means that <span class="math">\(\frac{dS}{dE} = 0\)</span>, so that the system is at either a maximum or minimum of entropy. (If you go through the math in more detail, it turns out that <span class="math">\(\beta = 0\)</span> implies a maximum of entropy.) Since negative value of <span class="math">\(T\)</span> means a negative value of <span class="math">\(\beta\)</span>, a negative temperature is achieved when, if by adding energy to the system, the entropy decreases. (In our switch model, this possibility is very clear. In our two-switch example, increasing the energy from <span class="math">\(E = 0\)</span> to <span class="math">\(E = 2\)</span> decreased the entropy since there were fewer microstates consistent with <span class="math">\(E = 2\)</span> than with <span class="math">\(E = 0\)</span>.) Adding energy to a real, typical system of particles, however, should always increase the entropy, so that <span class="math">\(\beta\)</span> (and hence <span class="math">\(T\)</span>) is always positive.</p>
<p>Physical principles do not outright disallow negative values of <span class="math">\(T\)</span>. It's just that we don't normally see systems which have that property. There are apparently some very particular quantum systems which have the property of a negative value of T, in the sense that increasing the total energy actually decreases the entropy. Likely, this is possible only for quantum systems which have a <strong>maximum</strong> allowable energy. Remember that energy is typically quantized in a quantum system, but there is often only a minimum energy, which increases in discrete amounts. A maximum energy, as far as I know, is not common. I studied quantum mechanics proper only up to a first-year graduate level, and all of the systems I encountered did not have a maximum allowable energy IIRC.</p>
<p>So this is right where my knowledge on the subject ends; I do not study condensed matter physics in any form. Hopefully, an expert on the subject can shed more light on how negative values of <span class="math">\(T\)</span> are actually achieved in a lab. But hopefully now you understand what a negative value of <span class="math">\(T\)</span> even means.</p>
<hr />
<p><em>the following addendum is by <a href="https://www.reddit.com/user/RobusEtCeleritas">/u/RobusEtCeleritas</a></em></p>
<p>The only thing I would add to this is that negative temperatures are only possible in systems where the total energy is bounded above. The easiest example to think of is the two-level system mentioned above. Physically, you can think of it as little spin-1/2 magnetic dipoles in a lattice (like the Ising model of ferromagnetism).</p>
<p>I actually like to think of the two-state paramagnet rather than the Ising model (the difference being that ferromagnets can keep their magnetization after the external magnetic field is shut off, but paramagnets can't).</p>
<p>Anyway, the two-state paramagnet has a maximum energy, and it's in the state where <strong>every single</strong> dipole is anti-aligned with the external magnetic field. The minimum energy is when every dipole is aligned with the external field.</p>
<p>So at the temperature <span class="math">\(0_+\)</span> (<span class="math">\(\beta = +\infty\)</span>), you'd expect every single dipole to be in the ground state, and at temperature <span class="math">\(0_-\)</span> (<span class="math">\(\beta = -\infty\)</span>), you'd expect every dipole to be anti-aligned. In the limit where the temperature goes to infinity (<span class="math">\(\beta\)</span> goes to zero), you'd reach maximum entropy, where all of the spins are oriented completely randomly.</p>
<p>It turns out that <strong>whenever</strong> more than half of the dipoles are anti-aligned with the external field, the absolute temperature is negative. That makes sense because as you decrease the energy (decrease the number of anti-aligned dipoles), you increase the entropy (or the &quot;randomness&quot; of the alignments). Since <span class="math">\(\frac{dS}{dE}\)</span> is negative, so is the temperature.</p>
<p>This is only possible because the two-level system has a maximum possible energy, and that's only possible because it's a quantum-mechanical system; you can't do this with a simple system like an ideal gas.</p>
<p>Now, how can you actually get the two-state paramagnet into a negative temperature state? Just cool it down to very near absolute zero. Now most of the spins are aligned with the external field.</p>
<p>Now reverse the direction of the external field. Suddenly, most of the dipoles are anti-aligned with the external field. They very much want to give away energy to the surroundings so they can align with the new direction of the field. Even though the system is technically at a negative temperature, it very much wants to give away energy (so it's very &quot;hot&quot;).</p>
<p>I mentioned before that the system is at a negative temperature whenever more than half of the dipoles are anti-aligned with the external field. Clearly in that configuration, the energy is higher than it would be if less than half of the dipoles were anti-aligned. So it follows that any negative temperature state of this system is necessarily &quot;hotter&quot; (higher total energy) than any state where the same system is at a positive temperature.</p>

			</div>
</div>
</body>
</html>
